{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9e7da2-9c18-4c15-9e08-bdb3010e4a81",
   "metadata": {},
   "source": [
    "## Introduction to Grad-CAM Analysis with TockyConvNetPy\n",
    "\n",
    "Welcome to our Jupyter notebook where we explore the application of Gradient-weighted Class Activation Mapping (Grad-CAM) using the `TockyConvNetPy` package. This analysis aims to provide insights into how different convolutional layers in our neural network model contribute to decisions made by the model, particularly in classifying and understanding complex image data.\n",
    "\n",
    "### Purpose of This Notebook\n",
    "\n",
    "- **Visualize Influential Regions:** Through Grad-CAM, we visualize the regions in the input images that are important for making predictions, which helps in understanding model behavior.\n",
    "- **Layer-by-Layer Analysis:** We analyze the influence of each convolutional layer to see how each one processes and transforms the input data to extract meaningful features.\n",
    "- **Model Assessment:** By examining the activation maps, we can assess whether the model is focusing on relevant parts of the images for making correct classifications, which is crucial for tasks such as medical imaging, object detection in autonomous vehicles, and more.\n",
    "\n",
    "### Model Overview\n",
    "\n",
    "The model used in this analysis is a convolutional neural network (ConvNet or CNN) that has been trained to classify images based on specific features learned during training. Below is a brief overview of the model architecture, highlighting the layers that will be analyzed:\n",
    "\n",
    "- **`conv2d`**: The first convolutional layer, applying initial filters to capture basic image features like edges.\n",
    "- **`conv2d_1`**: Follows `conv2d`, aimed at refining the features by applying a spatial attention mechanism.\n",
    "- **`conv2d_2`**: Starts the second block of convolution, processing deeper features.\n",
    "- **`conv2d_3`**: Similar to `conv2d_1`, it refines features for the second block.\n",
    "\n",
    "This notebook will guide you through loading the model, processing images, and visualizing the Grad-CAM heatmaps for these layers. We will also evaluate the model's performance on a set of test images and discuss the results.\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "To begin, ensure that the `TockyConvNetPy` package is installed in your environment. This package includes the pre-trained model and utilities necessary for generating and visualizing Grad-CAM heatmaps. \n",
    "\n",
    "In the current workflow, a pre-trained model will be loaded and used for Grad-CAM analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0432b5-adb9-4aae-9351-660130c38a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 13:49:47.141131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input (InputLayer)       [(None, 100, 100, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv1 (Conv2D)                 (None, 100, 100, 16  160         ['image_input[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " inst_norm1 (InstanceNormalizat  (None, 100, 100, 16  32         ['conv1[0][0]']                  \n",
      " ion)                           )                                                                 \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 13:49:50.480495: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " activation1 (Activation)       (None, 100, 100, 16  0           ['inst_norm1[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " attention1_conv (Conv2D)       (None, 100, 100, 1)  17          ['activation1[0][0]']            \n",
      "                                                                                                  \n",
      " attention1_multiply (Multiply)  (None, 100, 100, 16  0          ['activation1[0][0]',            \n",
      "                                )                                 'attention1_conv[0][0]']        \n",
      "                                                                                                  \n",
      " max_pool1 (MaxPooling2D)       (None, 50, 50, 16)   0           ['attention1_multiply[0][0]']    \n",
      "                                                                                                  \n",
      " dropout1 (Dropout)             (None, 50, 50, 16)   0           ['max_pool1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2 (Conv2D)                 (None, 50, 50, 32)   4640        ['dropout1[0][0]']               \n",
      "                                                                                                  \n",
      " activation2 (Activation)       (None, 50, 50, 32)   0           ['conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " attention2_conv (Conv2D)       (None, 50, 50, 1)    33          ['activation2[0][0]']            \n",
      "                                                                                                  \n",
      " attention2_multiply (Multiply)  (None, 50, 50, 32)  0           ['activation2[0][0]',            \n",
      "                                                                  'attention2_conv[0][0]']        \n",
      "                                                                                                  \n",
      " max_pool2 (MaxPooling2D)       (None, 25, 25, 32)   0           ['attention2_multiply[0][0]']    \n",
      "                                                                                                  \n",
      " dropout2 (Dropout)             (None, 25, 25, 32)   0           ['max_pool2[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 20000)        0           ['dropout2[0][0]']               \n",
      "                                                                                                  \n",
      " timer_pos_input (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concat (Concatenate)           (None, 20001)        0           ['flatten[0][0]',                \n",
      "                                                                  'timer_pos_input[0][0]']        \n",
      "                                                                                                  \n",
      " dense_fc (Dense)               (None, 128)          2560256     ['concat[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_fc (Dropout)           (None, 128)          0           ['dense_fc[0][0]']               \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 4)            516         ['dropout_fc[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,565,654\n",
      "Trainable params: 2,565,654\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from TockyConvNetPy import InstanceNormalization\n",
    "\n",
    "# Make sure the InstanceNormalization class is defined in the current scope.\n",
    "custom_objects = {\"InstanceNormalization\": InstanceNormalization}\n",
    "\n",
    "model = load_model(\"CNN2layersModel.keras\", custom_objects=custom_objects)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404653da-cff9-48b9-86ed-58499ce7f15c",
   "metadata": {},
   "source": [
    "## **Evaluating the TockyConvNet Model on an Independent Test Dataset**\n",
    "\n",
    "In this section, we assess the performance of the **TockyConvNet** model using an independent test dataset. The goal is to quantify the model's predictive capabilities and visualize its classification performance using **Receiver Operating Characteristic (ROC) curves** and **Precision-Recall (PR) curves**.\n",
    "\n",
    "### **Steps in This Analysis**\n",
    "1. **Load the Independent Test Data**  \n",
    "   - Load test images and labels stored in `data/dataset_CNS2/test_data/`.\n",
    "   - Extract the ground-truth class labels from the one-hot encoded format.\n",
    "\n",
    "2. **Model Predictions & Threshold Optimization**  \n",
    "   - Use the trained model to generate class probabilities for each test image.\n",
    "   - Compute the **ROC curve** and determine the **optimal threshold** for classification using the **geometric mean (G-mean)** method.\n",
    "\n",
    "3. **Performance Metrics Calculation**  \n",
    "   - Generate a **confusion matrix** to assess classification performance.\n",
    "   - Compute key **metrics**, including:\n",
    "     - **Accuracy**\n",
    "     - **Sensitivity (Recall)**\n",
    "     - **Specificity**\n",
    "     - **ROC-AUC score**\n",
    "\n",
    "4. **Visualizing Model Performance**  \n",
    "   - Plot the **ROC curve** to illustrate the trade-off between sensitivity and specificity.\n",
    "   - Generate the **Precision-Recall (PR) curve** to evaluate performance in handling class imbalances.\n",
    "\n",
    "The following code implements these steps, providing a detailed breakdown of the model’s effectiveness in classifying test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4b6749-754c-42c6-a17d-712742048b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 100, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 100, 100, 16  160         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 100, 100, 16  0           ['conv2d[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 100, 100, 1)  17          ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 100, 100, 16  0           ['activation[0][0]',             \n",
      "                                )                                 'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 50, 50, 16)   0           ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 50, 50, 16)   0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 50, 50, 16)   2320        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 50, 50, 16)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 50, 50, 1)    17          ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 50, 50, 16)   0           ['activation_1[0][0]',           \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 16)  0           ['multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 25, 25, 16)   0           ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 10000)        0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           320032      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            66          ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 322,612\n",
      "Trainable params: 322,612\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import importlib.resources\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Here, \"TockyConvNetPy.data.models\" is the package where your file is located.\n",
    "# And the resource is just the file name.\n",
    "with importlib.resources.path(\"TockyConvNetPy.data.models\", \"CNS2Foxp3Tocky_trained_model.keras\") as model_path:\n",
    "    model = load_model(str(model_path))\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21cd18-cd3d-402b-b5f4-f4ff463dd2a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Test Images From Independent Test Dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import TockyConvNetPy\n",
    "\n",
    "#def find_optimal_threshold(fpr, tpr, thresholds, factor=0.5):\n",
    "#    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "#    index = np.argmax(gmeans)\n",
    "#    return thresholds[index]\n",
    "#images_path = pkg_resources.resource_filename('TockyConvNetPy', 'data/dataset_CNS2/test_data/sample_images.npy')\n",
    "#labels_path = pkg_resources.resource_filename('TockyConvNetPy', 'data/dataset_CNS2/test_data/sample_labels.npy')\n",
    "\n",
    "import importlib.resources\n",
    "\n",
    "with importlib.resources.path(\"TockyConvNetPy.data.dataset_CNS2.test_data\", \"sample_images.npy\") as images_path:\n",
    "    with importlib.resources.path(\"TockyConvNetPy.data.dataset_CNS2.test_data\", \"sample_labels.npy\") as labels_path:\n",
    "        images_path = str(images_path)  # convert Path to string if needed\n",
    "        labels_path = str(labels_path)\n",
    "        # Now use images_path and labels_path as file paths\n",
    "\n",
    "\n",
    "sample_images = np.load(images_path)\n",
    "sample_labels = np.load(labels_path)\n",
    "true_classes = np.argmax(sample_labels, axis=1)\n",
    "\n",
    "# Make predictions and Compute ROC and PR curves\n",
    "probabilities = model.predict(sample_images)\n",
    "positive_class_probabilities = probabilities[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(true_classes, positive_class_probabilities)\n",
    "roc_auc = roc_auc_score(true_classes, positive_class_probabilities)\n",
    "\n",
    "optimal_threshold = find_optimal_threshold(fpr, tpr, thresholds)\n",
    "predicted_classes = (probabilities[:, 1] >= optimal_threshold).astype(int)\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Specificity and Sensitivity\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "specificity = TN / (TN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "print(f\"Specificity: {specificity:.2f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.2f}\")\n",
    "acc = accuracy_score(true_classes, predicted_classes)\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# Plotting ROC and PR curves\n",
    "precision, recall, _ = precision_recall_curve(true_classes, positive_class_probabilities)\n",
    "average_precision = average_precision_score(true_classes, positive_class_probabilities)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# PR Curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='PR curve (area = %0.2f)' % average_precision)\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2, color='blue')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65b5da-70e0-4fd2-b2f0-0115f98eaa5c",
   "metadata": {},
   "source": [
    "## Grad-CAM Analysis of Convolutional Layers\n",
    "\n",
    "This section details the Grad-CAM analysis performed on the different convolutional layers within our model. Grad-CAM allows us to visualize which areas of the input image are important for predicting the model's output by highlighting the most influential regions in each convolutional layer.\n",
    "\n",
    "### Analyzed Layers\n",
    "\n",
    "- **`conv2d`:** This is the first convolutional layer in our model. It applies 16 filters of size 3x3 to the input image, preserving the spatial dimensions (100x100), and is crucial for initial feature extraction such as edges and textures.\n",
    "\n",
    "- **`conv2d_1`:** Positioned right after `conv2d`, this layer implements spatial attention using a 1x1 convolution that outputs a single channel feature map. This map, through a sigmoid activation function, creates an attention mask that helps the model focus on relevant parts of the input in subsequent layers.\n",
    "\n",
    "- **`conv2d_2`:** This layer marks the beginning of the second convolutional block. Similar to `conv2d`, it further processes the features reduced in dimensionality by previous pooling layers, applying additional 16 filters to capture more complex patterns.\n",
    "\n",
    "- **`conv2d_3`:** Mirroring the function of `conv2d_1`, this layer applies a 1x1 convolution to generate a single channel spatial attention map within the second convolutional block. It focuses the model's learning on specific areas, improving the interpretability and effectiveness of the model.\n",
    "\n",
    "Each of these layers contributes uniquely to the model’s ability to accurately classify images by focusing on different types of features, from basic edges to more complex textural information.\n",
    "\n",
    "# Analysis Overview\n",
    "\n",
    "The following part analyses each of the convolutional layers of the TockyCNN model using Grad-CAM. Our goal is to identify which features are most influential in determining the output of the model and how different layers contribute to the decision-making process.\n",
    "\n",
    "## Steps:\n",
    "1. Load the model and display its summary.\n",
    "2. Process sample images through the model.\n",
    "3. Generate and visualize Grad-CAM heatmaps for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb884033-56a9-408f-900f-cc6971eb7cc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from TockyConvNetPy import find_optimal_threshold, generate_aggregated_heatmap, smooth_heatmap, visualize_heatmaps\n",
    "\n",
    "layer_names = [\"conv2d\", \"conv2d_1\", \"conv2d_2\", \"conv2d_3\"]\n",
    "\n",
    "for layer in layer_names:\n",
    "    probabilities = model.predict(sample_images)\n",
    "    WT_class_probabilities = probabilities[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(true_classes, WT_class_probabilities)\n",
    "    optimal_threshold_WT = find_optimal_threshold(fpr, tpr, thresholds)\n",
    "\n",
    "    KO_class_probabilities = probabilities[:, 0]\n",
    "    fpr, tpr, thresholds = roc_curve(true_classes, KO_class_probabilities)\n",
    "    optimal_threshold_KO = find_optimal_threshold(fpr, tpr, thresholds)\n",
    "\n",
    "    WT_aggregated_heatmap = generate_aggregated_heatmap(sample_images, model, layer, optimal_threshold_WT, 1)\n",
    "    CNS2KO_aggregated_heatmap = generate_aggregated_heatmap(sample_images, model, layer, optimal_threshold_KO, 0)\n",
    "\n",
    "    WT_smoothed_heatmap = smooth_heatmap(WT_aggregated_heatmap, sigma=3)\n",
    "    CNS2KO_smoothed_heatmap = smooth_heatmap(CNS2KO_aggregated_heatmap, sigma=3)\n",
    "    heatmap_difference = WT_smoothed_heatmap - CNS2KO_smoothed_heatmap\n",
    "\n",
    "    print(f\"Grad-CAM Analysis of {layer}\")\n",
    "\n",
    "    visualize_heatmaps(WT_aggregated_heatmap, WT_smoothed_heatmap, heatmap_difference, f\"WT Grad-CAM ({layer})\")\n",
    "    visualize_heatmaps(CNS2KO_aggregated_heatmap, CNS2KO_smoothed_heatmap, -heatmap_difference, f\"CNS2KO Grad-CAM ({layer})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20571ec1-9222-47b5-95f3-c0b1ac6ce385",
   "metadata": {},
   "source": [
    "## **Generating Grad-CAM Heatmaps for Downstream Analysis in TockyConvNetR**\n",
    "\n",
    "This section applies **Grad-CAM** to visualize the most influential regions of input images that contribute to classification decisions made by the **TockyConvNet** model. Specifically, we focus on **`conv2d_3`**, the final convolutional layer, as it captures high-level feature representations crucial for classification. \n",
    "\n",
    "The generated **Grad-CAM heatmaps** will be exported as CSV files and stored as **example datasets** in the **`inst/extdata`** directory of the **TockyConvNetR** package. These files will facilitate **R-based downstream analysis**, including visualization, statistical comparisons, and spatial feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "### **Workflow Overview**\n",
    "1. **Select the Last Convolutional Layer (`conv2d_3`)**  \n",
    "   - This layer is chosen as it represents the **final spatial feature map** before the fully connected layers.  \n",
    "   - Grad-CAM applied at this stage highlights **discriminative image regions** for each class (WT vs KO).  \n",
    "\n",
    "2. **Compute Class Probabilities & Determine Optimal Thresholds**  \n",
    "   - Model predictions are obtained for both **WT** and **KO** samples.  \n",
    "   - The **ROC curve** is used to determine the **optimal classification threshold** for each class.\n",
    "\n",
    "3. **Generate & Process Grad-CAM Heatmaps**  \n",
    "   - **Grad-CAM heatmaps** are computed for WT and KO sample sets.  \n",
    "   - These heatmaps are **Gaussian-smoothed** to reduce noise and enhance spatial coherence.  \n",
    "   - The **difference heatmap** between WT and KO is calculated for comparative analysis.\n",
    "\n",
    "4. **Export Processed Heatmaps to CSV**  \n",
    "   - The **heatmap difference matrix** is saved in CSV format.  \n",
    "   - These CSV files are included in **`inst/extdata`** of the **TockyConvNetR** package, allowing users to load them via R functions.  \n",
    "   - The exported data serves as **example datasets** for spatial and statistical analyses within R.\n",
    "\n",
    "---\n",
    "\n",
    "### **Note:**\n",
    "Using **Grad-CAM** at `conv2d_3`, we extract interpretable **spatial activation patterns** from deep learning models. The resulting heatmap data are indeed the one stored in **TockyConvNetR as an example heatmap data**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d846caa-06f4-4f1c-b733-1d21db389965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save results to CSV\n",
    "\n",
    "#from TockyConvNetPy find_optimal_threshold, generate_aggregated_heatmap, smooth_heatmap visualize_heatmaps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage import exposure\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Setup\n",
    "conv_layer = \"conv2d_3\" \n",
    "\n",
    "# Calculate probabilities and thresholds\n",
    "probabilities = model.predict(sample_images)\n",
    "WT_class_probabilities = probabilities[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(true_classes, WT_class_probabilities)\n",
    "optimal_threshold_WT = find_optimal_threshold(fpr, tpr, thresholds)\n",
    "\n",
    "KO_class_probabilities = probabilities[:, 0]\n",
    "fpr, tpr, thresholds = roc_curve(true_classes, KO_class_probabilities)\n",
    "optimal_threshold_KO = find_optimal_threshold(fpr, tpr, thresholds)\n",
    "\n",
    "WT_aggregated_heatmap = generate_aggregated_heatmap(sample_images, model, conv_layer, optimal_threshold_WT, 1)\n",
    "CNS2KO_aggregated_heatmap = generate_aggregated_heatmap(sample_images, model, conv_layer,  optimal_threshold_KO, 0)\n",
    "\n",
    "WT_smoothed_heatmap = smooth_heatmap(WT_aggregated_heatmap, sigma=3)\n",
    "CNS2KO_smoothed_heatmap = smooth_heatmap(CNS2KO_aggregated_heatmap, sigma=3)\n",
    "heatmap_difference = WT_smoothed_heatmap - CNS2KO_smoothed_heatmap\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_path = os.path.join(conv_layer, filename)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "if not os.path.exists(conv_layer):\n",
    "    os.makedirs(conv_layer)\n",
    "\n",
    "save_to_csv(heatmap_difference, 'heatmap_difference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee39c4a-5838-47d4-918f-d9dab76e7f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (legacy_tf)",
   "language": "python",
   "name": "legacy_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
